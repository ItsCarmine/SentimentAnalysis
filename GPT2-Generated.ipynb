{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "491845de-383b-4542-b7fb-5aa943cd5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14e24ede-e585-4508-9f3f-dff3ac12ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        for review, sentiment in zip(reviews, sentiments):\n",
    "            # Prepare sentiment control token\n",
    "            sentiment_token = \"<|pos|>\" if sentiment == \"positive\" else \"<|neg|>\"\n",
    "            # Combine sentiment token and review\n",
    "            text = f\"{sentiment_token} {review} <|endoftext|>\"\n",
    "            \n",
    "            # Encode the text\n",
    "            encodings = tokenizer(text, \n",
    "                                truncation=True,\n",
    "                                max_length=max_length,\n",
    "                                padding='max_length',\n",
    "                                return_tensors='pt')\n",
    "            \n",
    "            self.input_ids.append(encodings['input_ids'].squeeze())\n",
    "            self.attn_masks.append(encodings['attention_mask'].squeeze())\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attn_masks[idx],\n",
    "            'labels': self.input_ids[idx]  # For language modeling\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d1213ce-bf97-410c-825c-b50929de9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer_and_model():\n",
    "    # Load tokenizer and add special tokens\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # Add padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Add sentiment tokens\n",
    "    special_tokens = {'additional_special_tokens': ['<|pos|>', '<|neg|>']}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    # Load model and resize token embeddings\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Set pad token id for the model\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c95c87b-2cb6-4875-8330-dc9a4ba46096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, train_dataloader, val_dataloader, epochs=3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Average validation loss: {avg_val_loss}\")\n",
    "        \n",
    "        # Save model and tokenizer after each epoch\n",
    "        model_save_path = f'sentiment_gpt2_epoch_{epoch+1}'\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba34e395-e426-4d06-88e2-0197956ea3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "    # Load IMDB dataset\n",
    "    data = pd.read_csv(\"input/imdb_dataset.csv\")\n",
    "    train_data = data.iloc[:40000]\n",
    "    val_data = data.iloc[40000:45000]\n",
    "    \n",
    "    print(\"Preparing tokenizer and model...\")\n",
    "    # Prepare tokenizer and model\n",
    "    tokenizer, model = prepare_tokenizer_and_model()\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    # Create datasets\n",
    "    train_dataset = IMDBDataset(\n",
    "        train_data['review'].tolist(),\n",
    "        train_data['sentiment'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    val_dataset = IMDBDataset(\n",
    "        val_data['review'].tolist(),\n",
    "        val_data['sentiment'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"Creating dataloaders...\")\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,  # Reduced batch size to be safe\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,  # Reduced batch size to be safe\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    # Train the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print GPU memory info if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    train(model, tokenizer, train_dataloader, val_dataloader, epochs=3, device=device)  # Added tokenizer here\n",
    "    \n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2eed7d0-c87e-4328-8013-0999c0e27878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Preparing tokenizer and model...\n",
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carmine\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders...\n",
      "Starting training...\n",
      "Using device: cuda\n",
      "GPU Memory available: 17.17 GB\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [12:52<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.8572372406721116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1250/1250 [00:23<00:00, 53.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.739152767944336\n",
      "Model and tokenizer saved to sentiment_gpt2_epoch_1\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [12:36<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.7160199007749557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1250/1250 [00:18<00:00, 66.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.7153115202903746\n",
      "Model and tokenizer saved to sentiment_gpt2_epoch_2\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [25:39<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.6458864232122896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1250/1250 [00:52<00:00, 23.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.707614884853363\n",
      "Model and tokenizer saved to sentiment_gpt2_epoch_3\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8747e599-47f9-4710-9866-e176cac36501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, sentiment, prompt=None, max_length=100, \n",
    "                 temperature=0.7, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text with more controlled parameters\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare sentiment control token\n",
    "    sentiment_token = \"<|pos|>\" if sentiment == \"positive\" else \"<|neg|>\"\n",
    "    \n",
    "    # Combine sentiment token with prompt if provided\n",
    "    if prompt:\n",
    "        text = f\"{sentiment_token} {prompt}\"\n",
    "    else:\n",
    "        text = sentiment_token\n",
    "        \n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate text with more controlled parameters\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=temperature,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c90f897-77d9-4395-8c32-d3ed8aa3375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model_path = 'sentiment_gpt2_epoch_3'\n",
    "    tokenizer, model = load_model_and_tokenizer(model_path)    \n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Generate a large dataset\n",
    "    generated_df = generate_dataset(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        num_samples=10000,  # Will generate 5000 positive and 5000 negative reviews\n",
    "        device=device,\n",
    "        output_file=\"generated_reviews.csv\"\n",
    "    )\n",
    "    \n",
    "    # Print some example generations\n",
    "    print(\"\\nExample generations:\")\n",
    "    for sentiment in ['positive', 'negative']:\n",
    "        examples = generated_df[generated_df['sentiment'] == sentiment].sample(3)\n",
    "        print(f\"\\n{sentiment.upper()} examples:\")\n",
    "        for text in examples['review']:\n",
    "            print(f\"\\n{text}\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "144a0afc-60af-454f-bc70-2ae1afc9700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 5000 positive reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]C:\\Users\\Carmine\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 5000/5000 [57:38<00:00,  1.45it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 5000 negative reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:07:59<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated dataset saved to generated_reviews.csv\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 10000\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    5000\n",
      "negative    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average review length: 116.1279\n",
      "\n",
      "Example generations:\n",
      "\n",
      "POSITIVE examples:\n",
      "\n",
      "This film was just absolutely fantastic. I have been a fan of the TV series since I was little, and when I heard that the BBC were giving this film a second run it absolutely blew my mind! I found myself laughing so hard I would have to say something. The writing is wonderful, the cinematography fantastic in its own right but with an extra twist which makes the whole thing worth watching. A must see for anyone who likes science fiction or horror. I can't stress enough how much I enjoyed every minute (except for a few). The production values are also superb and the acting is top notch. I'm sure it will be on my DVD set soon though... <|endoftext|>\n",
      "================================================================================\n",
      "\n",
      "I saw this as an 8th Season on DVD with a 3 of the movie that is in my \"favorites\". One of those that I consider one of my favorites. <br /><br />The plot is quite intriguing and very intriguing. The character and what is the purpose for him do to go off to a different school in his small town are not entirely explained.<br />>The only thing that makes this show interesting is that the main character was a student at his high School but he's now at another highschool where there has been much better scholarship classes. It's also interesting to note that he didn't go back into college until later on so it isn´t clear whether he had gone thru anything that would have saved him from going to college. There was also the possibility that he had not had as much success playing college sports as before. But it appears they did put an end twist around that would be a perfect solution.<br/><br]Overall, this show is definitely\n",
      "================================================================================\n",
      "\n",
      "I liked it. The story is simple and entertaining, and the cast is excellent. It was a real treat to watch the early parts of the movie. The scenes between Mork (David Nesmith) as he listens in awe at how God has given them such power is just the tip of my iceberg as an actor playing a great role in this film.<br /><br />The first part on our minds that really hit home for me was \"The Story Of Mary\", the book about the journey of Peter Pan and her family. It is also one thing when you see those scenes that make me want to walk out of your room, and then sit through the rest of\n",
      "================================================================================\n",
      "\n",
      "NEGATIVE examples:\n",
      "\n",
      "I rented this movie because i thought it was going to be a horror movie, but the plot is so bad i actually lost interest. The only reason i watched it was to see how stupid it would become.<br /><br />The acting was horrible, and all the characters were basically caricatures of people who don't even have names (like a kid in \"Cinderella\") or are in fact dead (like the main character) and they're just annoying. The movie is about a girl who's friends get murdered by some crazy guy that has a really good gun, but when she tries something else, he uses his gun to kill her boyfriend (because he had the guns). She tries again, but this time the killer is not the murderer, but rather her friend's boyfriend. So instead we get another murder, and you get this girl trying things that aren´t even funny, like talking with someone who´s drunk and being mean towards her boyfriend...and thats it! This movie is NOT FUNNY. <|endoftext|>\n",
      "================================================================================\n",
      "\n",
      "This movie is really bad. If you are looking for a good 80's action horror movie, watch this one instead. As soon as you see the back of it, your brain goes \"Hurry up!\", because if the scriptwriter had been more subtle he could have made something that was better than this. I'm shocked to find out that other people liked this film. <|endoftext|>\n",
      "================================================================================\n",
      "\n",
      "I have to admit that this is my first review, which is a good sign. But it doesn't help because there's so much of this movie that I really wish it would come back and not be repeated.<br /><br />I'll give the spoilers here but you know what: the main problem with this movie was the bad acting. The script is terrible, the directing is bad, the characters are horrible, and the story line is awful (and possibly worse than some movies made in the last 50 years).<br />>I'm sorry but, when Hollywood tells us how to make an intelligent film, we're usually afraid of the same kind of crappy writing and directing. This movie doesn`t even do anything useful for these kinds people, its just bad. It has no purpose at all.<br/><br/>The premise is simple enough, but if you watch the trailer on Youtube, and see the actors playing the role like that, you will get the idea that this movie is trying too\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae1d8b3e-2b31-4f79-a5ca-cbd73f3fdad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"Load the saved model and tokenizer\"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    \n",
    "    # Set pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c574315-f63d-4073-8704-8a9023c08464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Sentiment Analysis of IMDB Movie Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:**\n",
    "\n",
    "In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51"
   },
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMDB Dataset.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ngame\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ngame\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ngame\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ngame\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import Levenshtein\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../SentimentAnalysis/input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary corpora\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be1b642cce343f7a8f68f8c91f7c50372cdf4381"
   },
   "source": [
    "**Import the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('../SentimentAnalysis/input/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ad3773974351ed9bdf389b2847d7475b36c2295"
   },
   "source": [
    "**Exploratery data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "453c3fd238f62ab8f649eb01771817e25bc0c77d"
   },
   "source": [
    "**Sentiment count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61964573faababe1f7897b77d32815a24954d2f"
   },
   "source": [
    "**Spliting the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d3aaabff555e07feb11c72cc3a6e457615975ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90da29c3b79f46f41d7391a2a116065b616d0fac"
   },
   "source": [
    "**Text normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f000c43d91f68f6668539f089c6a54c5ce3bd819"
   },
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "328b6e5977da3e055ad4b2e11a31e5e12ccf3b16"
   },
   "source": [
    "**Removing html strips and noise text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a"
   },
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88117b74761d1047924d6d70f76642faa0e706ac"
   },
   "source": [
    "**Removing special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "219da72b025121fd98081df50ae0fcaace10cc9d"
   },
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b66eeabd5b7b8c251f8b8ddf331140a64bcd514"
   },
   "source": [
    "**Text stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2295f2946e0ab74c220ad538d0e7adc04d23f697"
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e83107e4a281d84d7ae42b4e2c8d81b7ece438e4"
   },
   "source": [
    "**Removing stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5dbff82b4d2d188d8777b273a75d8ac714d38885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'or', 'needn', 'below', 'not', 'by', 'i', 'he', 'those', 'these', 'were', 'out', \"she's\", 'it', 'y', 'only', 'and', 'at', 'itself', 'herself', 'do', 'into', 'be', 'was', 'then', \"hasn't\", 'there', 'you', 'been', 'hadn', 'wasn', 'why', 'nor', 'that', 'have', 'before', 's', 'both', 'most', 'should', \"you've\", 'to', 'now', 'd', 'off', \"should've\", 'between', 'such', 'about', 'yourself', 'of', 'o', \"doesn't\", 'few', 'other', 'the', 'once', 'hers', 'its', 'ourselves', 're', 've', 'won', 'own', 'who', 'each', 'theirs', 'me', 'they', 'ours', 'all', 'while', 'can', \"wasn't\", 'don', \"mightn't\", 'our', 'themselves', 'had', 'himself', 'above', 'some', \"weren't\", 'she', 'through', \"shouldn't\", 'as', \"it's\", 'whom', 'ma', 'hasn', 'down', \"haven't\", \"won't\", 'shan', 'has', 'over', 'mustn', 'myself', \"you'd\", 'his', 'weren', 'during', 'how', 'on', 'him', 'so', 'doesn', 'too', 'their', 'your', 'having', \"don't\", 'mightn', 'my', 'for', 'couldn', 'because', 'aren', 'which', 'yourselves', 'her', 'will', \"wouldn't\", 'is', 'any', \"aren't\", 'further', 'am', \"didn't\", 'isn', \"mustn't\", 'here', \"hadn't\", 'did', 'a', 'than', 'this', 'wouldn', 'what', 'if', 'when', 'haven', 'very', \"that'll\", 'are', 'm', 'after', \"shan't\", 'again', 'an', 'doing', 'but', 'we', 'being', 'until', 'more', 'them', 'does', \"you'll\", 'same', 'yours', \"needn't\", 'under', 'with', 'just', 'no', 't', 'where', \"isn't\", \"you're\", \"couldn't\", 'from', 'in', 'up', 'll', 'shouldn', 'ain', 'against', 'didn'}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b35e7499291173119ed42287deac6f0cd96516e1"
   },
   "source": [
    "**Normalized train reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "#convert dataframe to string\n",
    "#norm_train_string=norm_train_reviews.to_string()\n",
    "#Spelling correction using Textblob\n",
    "#norm_train_spelling=TextBlob(norm_train_string)\n",
    "#norm_train_spelling.correct()\n",
    "#Tokenization using Textblob\n",
    "#norm_train_words=norm_train_spelling.words\n",
    "#norm_train_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d69462bb209a66cff86376dc8481d0c0140d894d"
   },
   "source": [
    "**Normalized test reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "c5d0d38bd9976150367e9d75f3b933774c96a1ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read review watch thi piec cinemat garbag took least 2 page find somebodi els didnt think thi appallingli unfunni montag wasnt acm humour 70 inde ani era thi isnt least funni set sketch comedi ive ever seen itll till come along half skit alreadi done infinit better act monti python woodi allen wa say nice piec anim last 90 second highlight thi film would still get close sum mindless drivelridden thi wast 75 minut semin comedi onli world semin realli doe mean semen scatolog humour onli world scat actual fece precursor joke onli mean thi handbook comedi tit bum odd beaver niceif pubesc boy least one hand free havent found playboy exist give break becaus wa earli 70 way sketch comedi go back least ten year prior onli way could even forgiv thi film even made wa gunpoint retro hardli sketch clown subtli pervert children may cut edg circl could actual funni come realli quit sad kept go throughout entir 75 minut sheer belief may save genuin funni skit end gave film 1 becaus wa lower scoreand onli recommend insomniac coma patientsor perhap peopl suffer lockjawtheir jaw would final drop open disbelief'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalized test reviews\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]\n",
    "# #convert dataframe to string\n",
    "# norm_test_string=norm_test_reviews.to_string()\n",
    "# # spelling correction using Textblob\n",
    "# norm_test_spelling=TextBlob(norm_test_string)\n",
    "# print(norm_test_spelling.correct())\n",
    "# # Tokenization using Textblob\n",
    "# norm_test_words=norm_test_spelling.words\n",
    "# norm_test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056900000000000006"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores('annoying')['compound'] - sid.polarity_scores('irritating')['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side\n",
      "Augmented Sentence: matchless reappraisal hour_angle citation lookout one oz episod youll bait right_field thi exactli find methamphetamine beginning matter strike oz Washington barbarous unflinch view violenc stage_set right_field news Adam faith thi display dim center diffident thi display wrench punch attentiveness drug sexual_activity violenc hardcor authoritative function wordit Call oz nicknam give Oswald utmost secur state_of_matter penitentari concentrate mainli emerald citi experiment part prison cellular_telephone field_glass front_man expression inbound privaci senior_high_school agenda em citi dwelling manyaryan Muslim gangsta Latin_American Christian Italian Irish moreso scuffl Death gaze dodgi batch shadi agreement never Army_for_the_Liberation_of_Rwanda awayi would state chief entreaty display ascribable fact goe display wouldnt make_bold forget pretti pictur key mainstream audienc forget appeal forget romanceoz doesnt fix about beginning episod always proverb strike nasti Washington phantasmagoric couldnt state Washington readi lookout evolve tast oz get habituate senior_high_school degree graphic violenc violenc injustic bend precaution wholl sell nickel inmat wholl killing ordering become aside good manner middl course inmat bend prison cunt ascribable miss street skill prison experi lookout oz May becom ease uncomfort viewingthat become contact dark slope\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def most_different_word_lexical(control_word, word_list):\n",
    "    differences = {}\n",
    "    for word in word_list:\n",
    "        distance = Levenshtein.distance(control_word, word)\n",
    "        differences[word] = distance\n",
    "\n",
    "    # Return the word with the highest distance\n",
    "    return max(differences, key=differences.get)\n",
    "def get_synonym(word):\n",
    "    \"\"\"Get a synonym for a word while preserving its sentiment.\"\"\"\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if not synonyms:\n",
    "        return word \n",
    "\n",
    "    syns = []\n",
    "    for synonym in synonyms:\n",
    "        lemma = synonym.lemmas()[0].name()\n",
    "        if lemma != word:  \n",
    "            syns.append(lemma)\n",
    "    if len(syns) == 0:\n",
    "        return word\n",
    "    return most_different_word_lexical(word, syns)\n",
    "\n",
    "def get_synonym_polar(word):\n",
    "    \"\"\"Get a synonym for a word while preserving its sentiment.\"\"\"\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if not synonyms:\n",
    "        return word  # Return the original word if no synonyms found\n",
    "    \n",
    "    original_score = sid.polarity_scores(word)['compound']\n",
    "    lemma_scores = {}\n",
    "    for synonym in synonyms:\n",
    "        lemma = synonym.lemmas()[0].name()  # Get the synonym\n",
    "        if lemma != word:  # Ensure it's not the same word\n",
    "            \n",
    "            synonym_score = sid.polarity_scores(lemma)['compound']\n",
    "            lemma_scores[lemma] = abs(original_score - synonym_score)\n",
    "            # Check if the sentiment score is close to the original\n",
    "            if abs(original_score - synonym_score) < 0.1:\n",
    "                return lemma\n",
    "    if lemma_scores:\n",
    "        syn = min(lemma_scores, key=lemma_scores.get)\n",
    "        return syn\n",
    "    return word\n",
    "# Test with a sentence\n",
    "sentence = 'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "augmented_tokens = [get_synonym_polar(token) for token in tokens]\n",
    "augmented_sentence = ' '.join(augmented_tokens)\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Augmented Sentence:\", augmented_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_reviews = []\n",
    "for review in norm_train_reviews:\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    augmented_train_reviews.append(' '.join([get_synonym(token) for token in tokens]))\n",
    "    \n",
    "augmented_test_reviews = []\n",
    "for review in norm_test_reviews:\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    augmented_test_reviews.append(' '.join([get_synonym(token) for token in tokens]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_reviews_p = []\n",
    "for review in norm_train_reviews:\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    augmented_train_reviews.append(' '.join([get_synonym_polar(token) for token in tokens]))\n",
    "    \n",
    "augmented_test_reviews_p = []\n",
    "for review in norm_test_reviews:\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    augmented_test_reviews.append(' '.join([get_synonym_polar(token) for token in tokens]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matchless reappraisal hour_angle citation lookout one oz episod youll bait right_field thi exactli find methamphetamine beginning matter strike oz Washington barbarous unflinch view violenc stage_set right_field news Adam faith thi display dim center diffident thi display wrench punch attentiveness drug sexual_activity violenc hardcor authoritative function wordit Call oz nicknam give Oswald utmost secur state_of_matter penitentari concentrate mainli emerald citi experiment part prison cellular_telephone field_glass front_man expression inbound privaci senior_high_school agenda em citi dwelling manyaryan Muslim gangsta Latin_American Christian Italian Irish moreso scuffl Death gaze dodgi batch shadi agreement never Army_for_the_Liberation_of_Rwanda awayi would state chief entreaty display ascribable fact goe display wouldnt make_bold forget pretti pictur key mainstream audienc forget appeal forget romanceoz doesnt fix about beginning episod always proverb strike nasti Washington phantasmagoric couldnt state Washington readi lookout evolve tast oz get habituate senior_high_school degree graphic violenc violenc injustic bend precaution wholl sell nickel inmat wholl killing ordering become aside good manner middl course inmat bend prison cunt ascribable miss street skill prison experi lookout oz May becom ease uncomfort viewingthat become contact dark slope',\n",
       " 'curiosity littl merchandise movie techniqu veri unassum veri oldtimebbc manner yield ease sometim discomfort pot reality entir piec actor extrem good Chosen Michael shininess onli hour_angle get polari hour_angle voic tap truli understand seamless edit guid mention william diari entri onli good Worth lookout terrificli write do piec maestro merchandise matchless bang-up maestro comedi hello animation reality realli semen dwelling littl matter fantasi precaution preferably function tradit ambition techniqu stay hearty vanish fun knowledg pot particularli view business orton halliwel stage_set particularli flatcar halliwel mural interior_decoration everi surfac terribl good make',\n",
       " 'idea thi Washington curiosity manner spend clock_time blistering summer weekend sit_down breeze condit dramaturgy lookout lightheart comedi diagram simplist dialogu witti charact likabl evening good boodle distrust series cause_of_death May disappoint realiz thi catch detail two hazard addict idea Washington validation woodi Allen hush fulli control_condition manner mani United_States turn lovethi Washington Idaho joke matchless woodi comedi class make_bold state decad ive never affect red johanson thi manag timbre sexi imag leap right_field averag emotional_state Young womanthi May Crown bejewel hello career Washington witty annoy clothing prada sake demigod bang-up comedi Adam understand Friend',\n",
       " 'BASIC famili littl male_child jake remember automaton hello cupboard hello rear battle timethi movi slow soap Opera suddenli jake decid becom rambo killing zombieok beginning Adam brand movie mustiness decid thriller play play movi watchabl rear divorc argu wish real_number animation jake hello cupboard sum destroy movie ask understand bogeyman alike movi alternatively lookout play meaningless thriller spots3 ten good fun rear origin dialogue shooting jake ignor',\n",
       " 'petter mattei beloved clock_time money ocular sandbag movie lookout Mister mattei crack United_States graphic portrayal homo relat thi movi look Tell United_States money office achiever peopl disagree situat encount thi variat Arthur schnitzler fun subject film_director transportation military_action show clock_time raw York disagree charact converge associate matchless associate matchless manner anoth following person matchless look acknowledge previou detail liaison stylishli movie hour_angle Sophist luxuri expression take understand peopl populate universe populate habitatth onli matter matchless become person pictur disagree phase loneli matchless populate large citi exactli Best topographic_point homo relat discovery sincer carry_through matchless spot event peopl encounterth work beneficial Mister mattei target steve buscemi Rosario Dawson carol kane Michael imperioli Hadrian grenier remainder endowment mold brand charact semen alivew regard Mister mattei beneficial fortune expect anxious hello following employment']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_reviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,)\n",
      "(40000, 1)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"normal_train_reviews.csv\"\n",
    "aug_train_df = pd.DataFrame(norm_train_reviews)\n",
    "aug_train_df.to_csv(output_path, index=False) \n",
    "\n",
    "output_path = \"normal_test_reviews.csv\"\n",
    "aug_test_df = pd.DataFrame(norm_test_reviews)\n",
    "aug_test_df.to_csv(output_path, index=False) \n",
    "\n",
    "output_path = \"augmented_train_reviews.csv\"\n",
    "aug = pd.Series(augmented_train_reviews)\n",
    "print(aug.shape)\n",
    "aug.name = 'review'\n",
    "aug_train_df = pd.DataFrame(aug)\n",
    "print(aug_train_df.shape)\n",
    "aug_train_df.to_csv(output_path, index=False) \n",
    "\n",
    "output_path = \"augmented_test_reviews.csv\"\n",
    "aug = pd.Series(augmented_test_reviews)\n",
    "aug.name = 'review'\n",
    "aug_test_df = pd.DataFrame(aug)\n",
    "aug_test_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,)\n",
      "(40000, 1)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"augmented_train_reviews_p.csv\"\n",
    "aug = pd.Series(augmented_train_reviews_p)\n",
    "print(aug.shape)\n",
    "aug.name = 'review'\n",
    "aug_train_df = pd.DataFrame(aug)\n",
    "print(aug_train_df.shape)\n",
    "aug_train_df.to_csv(output_path, index=False) \n",
    "\n",
    "output_path = \"augmented_test_reviews_p.csv\"\n",
    "aug = pd.Series(augmented_test_reviews_p)\n",
    "aug.name = 'review'\n",
    "aug_test_df = pd.DataFrame(aug)\n",
    "aug_test_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    matchless recapitulation hour_angle note deter...\n",
       "1    curiosity littl intersection movie techniqu ve...\n",
       "2    remember thi Washington curiosity direction sp...\n",
       "3    BASIC famili littl male_child jake remember au...\n",
       "4    petter mattei sleep_together fourth_dimension ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one review ha mention watch 1 oz episod youll ...\n",
       "1    wonder littl product film techniqu veri unassu...\n",
       "2    thought thi wa wonder way spend time hot summe...\n",
       "3    basic famili littl boy jake think zombi hi clo...\n",
       "4    petter mattei love time money visual stun film...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    beginning privation state tilt liber polit pla...\n",
       "1    Washington excit understand situation_comedy w...\n",
       "2    expression screen take material entir disagree...\n",
       "3    wish mani consider look denni hop-picker brand...\n",
       "4    thi movi Washington television sidereal_day di...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('one.n.01'),\n",
       " Synset('one.n.02'),\n",
       " Synset('one.s.01'),\n",
       " Synset('one.s.02'),\n",
       " Synset('one.s.03'),\n",
       " Synset('one.s.04'),\n",
       " Synset('one.s.05'),\n",
       " Synset('one.s.06'),\n",
       " Synset('matchless.s.01')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"echo -e '\\a'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

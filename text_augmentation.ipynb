{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d3f075-0b70-4cf7-8d28-38b6fcc3e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea01c1d-068f-4255-b164-bbe111df9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAugmenter:\n",
    "    \"\"\"Base class for text augmentation\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"base\"\n",
    "    \n",
    "    def augment(self, text: str) -> str:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        return self.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b681a4cd-2024-4b15-8f4d-cae26db16ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynonymAugmenter(BaseAugmenter):\n",
    "    \"\"\"Augments text by replacing words with synonyms\"\"\"\n",
    "    def __init__(self, aug_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.name = \"synonym\"\n",
    "        # Using PPDB (Paraphrase Database) for synonym replacement\n",
    "        self.aug = naw.SynonymAug(\n",
    "            aug_p=aug_p,  # Percentage of words to replace\n",
    "            aug_min=1     # Minimum number of words to replace\n",
    "        )\n",
    "    \n",
    "    def augment(self, text: str) -> str:\n",
    "        return self.aug.augment(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b5c3da-6738-4dc7-89c5-9db647d0da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTranslationAugmenter(BaseAugmenter):\n",
    "    \"\"\"Augments text using back translation\"\"\"\n",
    "    def __init__(self, source_lang=\"en\", intermediate_lang=\"fr\"):\n",
    "        super().__init__()\n",
    "        self.name = \"backtranslation\"\n",
    "        # Load translation models\n",
    "        self.source_lang = source_lang\n",
    "        self.intermediate_lang = intermediate_lang\n",
    "        \n",
    "        # Initialize translation models\n",
    "        self.model_forward = MarianMTModel.from_pretrained(\n",
    "            f'Helsinki-NLP/opus-mt-{source_lang}-{intermediate_lang}'\n",
    "        )\n",
    "        self.tokenizer_forward = MarianTokenizer.from_pretrained(\n",
    "            f'Helsinki-NLP/opus-mt-{source_lang}-{intermediate_lang}'\n",
    "        )\n",
    "        \n",
    "        self.model_backward = MarianMTModel.from_pretrained(\n",
    "            f'Helsinki-NLP/opus-mt-{intermediate_lang}-{source_lang}'\n",
    "        )\n",
    "        self.tokenizer_backward = MarianTokenizer.from_pretrained(\n",
    "            f'Helsinki-NLP/opus-mt-{intermediate_lang}-{source_lang}'\n",
    "        )\n",
    "    \n",
    "    def translate(self, texts: List[str], model: MarianMTModel, tokenizer: MarianTokenizer) -> List[str]:\n",
    "        tokens = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "        translate_tokens = model.generate(**tokens)\n",
    "        return [tokenizer.decode(t, skip_special_tokens=True) for t in translate_tokens]\n",
    "    \n",
    "    def augment(self, text: str) -> str:\n",
    "        try:\n",
    "            # Forward translation\n",
    "            intermediate = self.translate([text], self.model_forward, self.tokenizer_forward)[0]\n",
    "            # Backward translation\n",
    "            augmented = self.translate([intermediate], self.model_backward, self.tokenizer_backward)[0]\n",
    "            return augmented\n",
    "        except:\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c28120-a4ad-450e-b866-b83be85c3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDeletionAugmenter(BaseAugmenter):\n",
    "    \"\"\"Augments text by randomly deleting words\"\"\"\n",
    "    def __init__(self, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.name = \"deletion\"\n",
    "        self.p = p\n",
    "    \n",
    "    def augment(self, text: str) -> str:\n",
    "        words = text.split()\n",
    "        if len(words) == 1:\n",
    "            return text\n",
    "        \n",
    "        # Randomly delete words with probability p\n",
    "        remaining_words = [word for word in words if random.random() > self.p]\n",
    "        \n",
    "        if not remaining_words:\n",
    "            # Keep at least one word\n",
    "            remaining_words = [random.choice(words)]\n",
    "        \n",
    "        return \" \".join(remaining_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a3895d-4b76-4aaa-9ed8-60d072f35580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceShuffleAugmenter(BaseAugmenter):\n",
    "    \"\"\"Augments text by shuffling sentences\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"shuffle\"\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    def augment(self, text: str) -> str:\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return text\n",
    "        \n",
    "        # Shuffle sentences\n",
    "        random.shuffle(sentences)\n",
    "        return \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f202c1-0864-4831-90f4-bcc80ba8f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboAugmenter:\n",
    "    \"\"\"Applies a fixed combination of augmentations\"\"\"\n",
    "    def __init__(self, combo_name: str = \"SBDR\"):\n",
    "        self.augmenters = []\n",
    "        self.combo_name = combo_name\n",
    "        \n",
    "        # Map letters to augmenters\n",
    "        aug_map = {\n",
    "            'S': SynonymAugmenter(),\n",
    "            'B': BackTranslationAugmenter(),\n",
    "            'D': RandomDeletionAugmenter(),\n",
    "            'R': SentenceShuffleAugmenter()\n",
    "        }\n",
    "        \n",
    "        # Initialize augmenters based on combo name\n",
    "        for letter in combo_name:\n",
    "            if letter in aug_map:\n",
    "                self.augmenters.append(aug_map[letter])\n",
    "    \n",
    "    def augment(self, text: str) -> List[str]:\n",
    "        \"\"\"Apply each augmentation in sequence\"\"\"\n",
    "        augmented_texts = []\n",
    "        current_text = text\n",
    "        \n",
    "        for augmenter in self.augmenters:\n",
    "            try:\n",
    "                current_text = augmenter(current_text)\n",
    "                augmented_texts.append(current_text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {augmenter.name}: {str(e)}\")\n",
    "                augmented_texts.append(text)  # Use original text if augmentation fails\n",
    "        \n",
    "        return augmented_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a628458-c826-4c48-b15d-cca590b1a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentor:\n",
    "    \"\"\"Main class for handling data augmentation in active learning\"\"\"\n",
    "    def __init__(self, combinations: List[str] = [\"SBDR\", \"SBD\", \"SDR\", \"BDR\"]):\n",
    "        self.combinations = combinations\n",
    "        self.combo_augmenters = [ComboAugmenter(combo) for combo in combinations]\n",
    "    \n",
    "    def augment_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Apply all combinations to a single text\"\"\"\n",
    "        all_augmented = []\n",
    "        for augmenter in self.combo_augmenters:\n",
    "            all_augmented.extend(augmenter.augment(text))\n",
    "        return all_augmented\n",
    "    \n",
    "    def augment_dataset(self, df: pd.DataFrame, text_column: str = 'review') -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Augment entire dataset and maintain mapping of originals to augmentations\"\"\"\n",
    "        augmentation_map = {}\n",
    "        all_augmented_texts = []\n",
    "        original_indices = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            text = row[text_column]\n",
    "            augmented = self.augment_text(text)\n",
    "            augmentation_map[idx] = augmented\n",
    "            all_augmented_texts.extend(augmented)\n",
    "            original_indices.extend([idx] * len(augmented))\n",
    "        \n",
    "        # Create DataFrame with augmented texts\n",
    "        augmented_df = pd.DataFrame({\n",
    "            'original_index': original_indices,\n",
    "            text_column: all_augmented_texts\n",
    "        })\n",
    "        \n",
    "        return augmented_df, augmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b118b17-a766-48af-b751-a5f4aac4b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "This an extremely horrible movie. And if your thinking, you've seen another horrible movie exactly like this one before you probably have. You probably, saw Scarecrow 2 made in 2003. Yes thats what I said Brian (the director) stole the movie idea.\n",
      "\n",
      "Augmented Versions:\n",
      "\n",
      "Version 1:\n",
      "This an super atrocious movie. And if your thinking, you ' ve seen another horrible picture show exactly like this one before you probably have. You probably, see Straw man 2 made in 2003. Yes thats what I suppose Brian (the director) stole the movie idea.\n",
      "\n",
      "Version 2:\n",
      "It's a terrible movie. And if you think, you saw another horrible picture showing exactly like this before you probably had. You probably, see Straw man 2 made in 2003. Yes, that's what I assume Brian (the director) stole the idea of the movie.\n",
      "\n",
      "Version 3:\n",
      "It's a movie. And if you think, you another horrible picture showing exactly like this before you probably had. You probably, see Straw man 2 made in 2003. Yes, what I assume Brian (the director) stole idea of the movie.\n",
      "\n",
      "Version 4:\n",
      "You probably, see Straw man 2 made in 2003. And if you think, you another horrible picture showing exactly like this before you probably had. Yes, what I assume Brian (the director) stole idea of the movie. It's a movie.\n",
      "\n",
      "Version 5:\n",
      "This an extremely horrible moving picture show. And if your thinking, you ' ve seen another horrible movie precisely like this one before you probably have. You probably, figure Straw man 2 made in 2003. Yes thats what I said Brian (the director) steal the movie idea.\n",
      "\n",
      "Version 6:\n",
      "And if you think, you saw another horrible movie exactly like this before you probably had. You probably figure Straw man 2 made in 2003. Yes that's what I said Brian (the director) stole the idea of the movie.\n",
      "\n",
      "Version 7:\n",
      "And if you think, you saw another horrible movie exactly like this before you probably had. You probably figure Straw man 2 made in Yes that's what I said Brian (the director) stole the of the movie.\n",
      "\n",
      "Version 8:\n",
      "This an highly horrible movie. And if your thinking, you ' ve seen another horrible motion picture show exactly like this i before you probably have. You probably, see Scarecrow 2 made in 2003. Yes thats what I said Brian (the director) stole the film thought.\n",
      "\n",
      "Version 9:\n",
      "This highly horrible movie. And if your thinking, you ' ve seen another horrible motion show exactly like this i before you probably have. You probably, see Scarecrow 2 made in 2003. Yes thats what said Brian (the director) stole film thought.\n",
      "\n",
      "Version 10:\n",
      "And if your thinking, you ' ve seen another horrible motion show exactly like this i before you probably have. You probably, see Scarecrow 2 made in 2003. Yes thats what said Brian (the director) stole film thought. This highly horrible movie.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Here we are just testing some example text augmentations so we have some examples for our presentation.\"\"\"\n",
    "# Load dataset\n",
    "df = pd.read_csv('datasets/text_augmentation_example.csv')\n",
    "    \n",
    "# Initialize augmentor with fixed combinations\n",
    "augmentor = DataAugmentor(combinations=[\"SBDR\", \"SBD\", \"SDR\"])\n",
    "    \n",
    "augmented_df, aug_map = augmentor.augment_dataset(df)\n",
    "    \n",
    "# Print original and augmented texts for the example csv\n",
    "print(\"Original Text:\")\n",
    "print(df.iloc[0]['review'])\n",
    "print(\"\\nAugmented Versions:\")\n",
    "for idx, aug_text in enumerate(aug_map[0]):\n",
    "    print(f\"\\nVersion {idx + 1}:\")\n",
    "    print(aug_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6897929e-9a1c-4dbc-9534-dd3b41cdfa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Carmine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Carmine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In the above example, you might run into a resource error where we cannot find the punkt_tab. In that case, please run the following code\"\"\"\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487c123-e9ee-47cf-ad22-d6220dc2ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
